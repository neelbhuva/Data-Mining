{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We will perform all different types of analysis on data which include:\n",
    "1. Doing bar plots and histograms based on the type of attribute.\n",
    "2. Counting the categories in each dataset.\n",
    "3. Looking at the missing values and imputing them with values that seem fit.\n",
    "4. Making assumptions for calculating the proximity measures on these attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "a. Instead of plotting the graphs independently for each variale, we will plot the graphs all in a single figure.\n",
    "b. We will plot barplots and histograms based on the type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.boxplot(\"education_cat\", by = \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* People with highest education tend to receive salary more than 50K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.boxplot(\"education_cat\", by = \"gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While there is equal spread in between education grade in Females, the highest education i.e. Doctorate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.boxplot(\"hour_per_week\", by = \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hours per week surely has an important role in predicting the salary. i.e. below or above 50K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.boxplot(\"age\", by = \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Although there is not much information available from this plot, it seems people get pay above 50k only after a certain age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# fig = plt.figure(figsize=(20,15))  #Creating a new figure with the mentioned figure size\n",
    "# cols = 4         # No of columns to display the charts\n",
    "# rows = 4         # No of rows to display the charts. These numbers are chosen as we have 16 attributes\n",
    "#                  # Alternatively, *rows = math.ceil(float(df.shape[1]) / cols)* \n",
    "#                  # can be used when there are indefinete number of attributes.\n",
    "\n",
    "# for i, column in enumerate(df.columns):\n",
    "#     ax = fig.add_subplot(rows, cols, i+1)      #Adds a subplot in the i+1 th position\n",
    "#     ax.set_title(column)\n",
    "#     if df.dtypes[column] == np.object:         #For categorical attributes.\n",
    "#         df[column].value_counts().plot(kind=\"bar\", axes=ax)\n",
    "#     else:\n",
    "#         df[column].hist(axes=ax)               #For conitnous attributes\n",
    "#         plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "# plt.subplots_adjust(hspace=1.2, wspace=0.2)    # To adjust the plots and their labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data:\n",
    "These charts give information about all the attributes.\n",
    "1.  Beginning with the first plot, number of people working in a particular age group is decreasing.\n",
    "2.  In the workclass graph, we can see a value marked with \"?\" in the workclass graph. This might be a missing value. Many of       the workclass records are Private.\n",
    "3.  In the education plot, we can see that many people have their highest education as HS-grad.\n",
    "4.  In education_cat, \"9\" is the most frequent record which seems same as the frequency of HS-grad in education. Seems like         education_cat is just a numeric to the education. We can confirm it with value_counts function.\n",
    "5.  Marital_status and relationship seem to be related as Married implies the person is a husband or wife. Never married relates     to unmarried.\n",
    "6.  There seems to be missing values in occupation marked with \"?\". And their frequency is somewhat similar to the missing           values in workclass. They might be missing values of the same record. We can validate this.\n",
    "7.  In the race category, majority of the observations are whites.\n",
    "8.  There are around 340 Males which constitute 65% of the data.\n",
    "9.  There are a lot of zeroes in the attributes capital_gain and capital_loss.\n",
    "10. From the hour_per_week graph, it seems there are a lot of observations in the bin 30-40. While calculating the similarities, \n",
    "    we can treat hour_per_week as coninuous or we can categorize it by implying 0-20 as part time, 20-40 as full time and rest       as over time.\n",
    "11. From the native_country, most frequent outcome is United-States and there are missing values which are represented as \"?\".       In addition to that, we can also infer that whites belong to United-States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"education\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"education_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can confirm that they both are the same and drop the education attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.drop(\"education\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To check the number of categories\n",
    "# df[\"marital_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"relationship\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see the sum of wife and husban is almost equal to Married-civ-spouse. We can say that these two variables are somewhat related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Majority of the observations are male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"race\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values:\n",
    "\n",
    "* Exploring further into the attributes workclass, occupation and native_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exploring the missing values in workclass\n",
    "# df[\"workclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see there are 28 missing values.\n",
    "* Now, exploring the missing values in occupation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this attribute, there are again 28 records missing. Probably the missing values are inside the same records. We can simply check this by accessing those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[df[\"occupation\"]==\" ?\"].index == df[df[\"workclass\"] == \" ?\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see missing values in both the attributes belong to the same records. Maybe this implies it is a an occupation which could not be enclosed or they might be unemployed. So we will just replace the values with a seperate category. We can give workclass as \"X\"(No proper meaning, only for the sake of categorizing the missing value) and \"Job_X\" for the occupation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_missing(df):\n",
    "    df[\"occupation\"].replace(to_replace =\"\\?\" , value = \"Job_X\", inplace = True, regex = True)\n",
    "    df[\"workclass\"].replace(to_replace=\"\\?\", value = \"X\", inplace = True, regex=True)\n",
    "    df[\"native_country\"].replace(to_replace = \"\\?\", value = \"United-States\", inplace = True, regex = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the columns once again to verify our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"workclass\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Both of the missing values are replaced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us take a look at the native_country attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"native_country\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since there are only 6 missing values, we can simply impute the most frequent value into these missing values. The most frequent one is the united States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[\"native_country\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see, the missing values are merged into United-States. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Attributes:\n",
    "\n",
    "* As we are now done dealing with the missing values, we will now decide how to deal with the attributes to calculate the proximity measure.\n",
    "\n",
    "* Continuous attributes:\n",
    "    * We have 5 continuous attributes age, fnlwgt, capital_gain, capital_loss.\n",
    "    * Capital_gain and capital_loss have a lot of zeroes in the data. We cannot use a distance measure to calculate the               similarities between them as 0-0 match does not mean anything. Hence, we have decided to label the records in capital_gain       and capital_loss as 0 for 0 and 1 for any other number. This will help us perform jaccard similarity on this data set as         it can be treated as a binary vector.\n",
    "    * To calculate distances between age, fnlwgt and hour_per_week we will have to scale the attributes. The feature scaling is       done to make the assumption that all the features has the equal opportunity to influence the weight, which more really           reflects the knowledge you know about the data. Therefore, we will scale these attributes in between 0 to 1. We can also         normalize the data, but for the sake of simplicity we are only scaling the data to 0 to 1 for all the three attributes.         Once we are done with the scaling, we calculated distance using Euclidian distance measure with L2 norm.\n",
    "* Nominal Attributes: \n",
    "    * Most of the attributes in this data set are nominal.\n",
    "    * For nominal attributes we simply use the similarity measure of s = 1 if both the records are equal, s = 0 if they are not       equal\n",
    "    * So the nominal attributes in the data are workclass, occupation, marital_status, gender, race, native_country,                   relationship. We will just use this measure for all these attributes.\n",
    "* Ordinal Attributes:\n",
    "    * There is only one ordinal attribute in this dataset and that is education_cat. education and education_cat both represent same data and we will drop education attribute as it is not useful for calculating the similarities. For ordinal data, we will map the data to 0 to 1 and then calculate the difference of these attributes. Similiarity will be 1 - d.\n",
    "\n",
    "\n",
    "\n",
    "## Proximity Measures for attributes:\n",
    "We will discuss about the different similarity or distance measures used for different attributes.\n",
    "\n",
    "### Continuous attributes: \n",
    "The continuous attributes that are in the data set are age, fnlwgt, hour_per_week. We will standadize the data using standard deviation scaling function. We can calculate the distances using Minkowski distance for n = 3 and L2 norm.\n",
    "\n",
    "Dissimilarity: d = minkowski with norm 2, similairty: s = 1/e^d. As the distance increases, e^d increases and the similarity decreases.\n",
    "\n",
    "### Ordinal Attributes:\n",
    "\n",
    "Dissimilarity d = |x-y|/(n-1) and similarity s = 1 - d.\n",
    "\n",
    "### Nominal Attributes:\n",
    "The nominal attributes in the data are workclass, education, occupation, marital_status, relationship, race, gender, native_country.\n",
    "\n",
    "Dissimilarity d = 0 if x = y, or d = 1 if x != y, \n",
    "similarity s = 1 if x = y, or s = 0 if x != y.\n",
    "\n",
    "### Scaling the data:    \n",
    "* We will scale the attributes, hour_per_week, age and fnlwgt in between 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    df[\"age\"] = df[[\"age\"]].apply(lambda x : (x - np.min(x))/(np.max(x) - np.min(x)))\n",
    "    df[\"fnlwgt\"] = df[[\"fnlwgt\"]].apply(lambda x : (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "    df[\"hour_per_week\"] = df[[\"hour_per_week\"]].apply(lambda x : (x - np.min(x)) / (np.max(x)-np.min(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To verify the data:\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see the values in the continuous attributes are less than 1. Now we can calculate the euclidian distance on these attributes.\n",
    "* We will create a different data frame just for the continuous attributes and apply the function on this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_euc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.boxplot(\"age\",by = \"relationship\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidian Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mdist(a,b,df_euc):                                      #mdist is the function to calculate euclidian distance with L2 norm\n",
    "    total = 0;\n",
    "    for i in range(0,len(df_euc.keys())):\n",
    "        diff = b[i] - a[i];\n",
    "        total += diff * diff;\n",
    "    return np.sqrt(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccardian Similarity Function:\n",
    "* This function counts the number of (0,1), (1,0) and (1,1) given two input vectors.\n",
    "* Calculates the jaccardian similarity for attributes listed in labels. Uses the jaccardian formula to get the similarity. Produces one value for all attributes in labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccardian_counts(vector_1,vector_2):\n",
    "    M11 = 0\n",
    "    M10 = 0\n",
    "    M01 = 0\n",
    "    for i in range(0,len(vector_1)):\n",
    "        if(vector_1[i] == 1 and vector_2[i] == 1):\n",
    "            M11 = M11 + 1\n",
    "        if(vector_1[i] == 1 and vector_2[i] == 0):\n",
    "            M10 = M10 + 1\n",
    "        if(vector_1[i] == 0 and vector_2[i] == 1):\n",
    "            M01 = M01 + 1\n",
    "    return M11,M10,M01\n",
    "\n",
    "def jaccardian_sim(labels,df_train,df_test):\n",
    "    delta_k = []\n",
    "    jacc_sim = []\n",
    "    length = num_rows\n",
    "    for j in range(0,length):\n",
    "        for k in range(0,num_test_rows):\n",
    "            vector_1 = []\n",
    "            vector_2 = []\n",
    "            for i in range(0,len(labels)):\n",
    "                vector_1.append(df_train[labels[i]][j])\n",
    "                vector_2.append(df_test[labels[i]][k])\n",
    "            M11,M10,M01 = jaccardian_counts(vector_1,vector_2)\n",
    "            delta_k.append(get_delta(vector_1,vector_2))\n",
    "            if(M11+M10+M01 == 0):\n",
    "                jacc_sim.append(0)\n",
    "                continue\n",
    "            jacc_sim.append(M11/(M10+M01+M11))\n",
    "            \n",
    "    ar = np.array(jacc_sim)\n",
    "    shape = (length,num_test_rows)\n",
    "    ar_reshape = ar.reshape(shape,)\n",
    "    df_sim = pd.DataFrame(data = ar_reshape)\n",
    "    \n",
    "    ar = np.array(delta_k)\n",
    "    shape = (length,num_test_rows)\n",
    "    ar_reshape = ar.reshape(shape,)\n",
    "    df_delta_k = pd.DataFrame(data = ar_reshape)\n",
    "    return(df_sim,df_delta_k)\n",
    "\n",
    "def get_norm(x):\n",
    "    square_sum = 0\n",
    "    for i in x:\n",
    "        square_sum = square_sum + i * i\n",
    "    return math.sqrt(square_sum)\n",
    "\n",
    "def dot(v1, v2):\n",
    "    sum_prod = 0\n",
    "    for i in range(0,len(v1)):\n",
    "        sum_prod = sum_prod + v1[i] * v2[i]\n",
    "    return sum_prod\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    dot_prod = dot(a,b)\n",
    "    #print(dot_prod)\n",
    "    a_norm = get_norm(a)\n",
    "    b_norm = get_norm(b)\n",
    "    #print(a_norm)\n",
    "    if(a_norm == 0 or b_norm == 0):\n",
    "        return 0\n",
    "    return (dot_prod/(a_norm * b_norm))\n",
    "\n",
    "\n",
    "def cosine_sim(labels):\n",
    "    delta_k = []\n",
    "    cosine_sim1 = []\n",
    "    length = num_rows\n",
    "    for j in range(0,length):\n",
    "        for k in range(0,length):\n",
    "            vector_1 = []\n",
    "            vector_2 = []\n",
    "            for i in range(0,len(labels)):\n",
    "                vector_1.append(df[labels[i]][j])\n",
    "                vector_2.append(df[labels[i]][k])\n",
    "            cosine_sim1.append(cos_sim(vector_1,vector_2))\n",
    "            delta_k.append(get_delta(vector_1,vector_2))\n",
    "            \n",
    "    ar = np.array(cosine_sim1)\n",
    "    shape = (length,length)\n",
    "    ar_reshape = ar.reshape(shape,)\n",
    "    df_sim = pd.DataFrame(data = ar_reshape)\n",
    "    \n",
    "    ar = np.array(delta_k)\n",
    "    shape = (length,length)\n",
    "    ar_reshape = ar.reshape(shape,)\n",
    "    df_delta_k = pd.DataFrame(data = ar_reshape)\n",
    "    return(df_sim,df_delta_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delta:\n",
    "* Given two vectors this function returns 0 if both vectors are zero vectors. Used for sparse data. Delta refers to variable delta in the general approach formula to calculate similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_delta(vector_1,vector_2):\n",
    "    for i in range(0,len(vector_1)):\n",
    "        if(vector_1[i] == 1 or vector_2[i] == 1):\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity function for Nominal and Ordinal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nom_sim(dframe_train,dframe_test):\n",
    "    temp = []\n",
    "    for i in dframe_train:\n",
    "        for j in dframe_test:\n",
    "            if i == j:\n",
    "                a = 1\n",
    "                temp.append(a)\n",
    "            else:\n",
    "                a = 0\n",
    "                temp.append(a)        \n",
    "    ar = np.array(temp)\n",
    "    shape = (len(dframe_train), len(dframe_test))\n",
    "    ar_reshape = ar.reshape(shape,)\n",
    "    df_sim = pd.DataFrame(data = ar_reshape)\n",
    "    return(df_sim)\n",
    "\n",
    "\n",
    "def ord_sim(dframe_train,dframe_test):\n",
    "    temp = []\n",
    "    for i in dframe_train:\n",
    "        i = i - 1\n",
    "        for j in dframe_test:\n",
    "            j = j - 1\n",
    "            d = abs(i - j) / (len(dframe_train.unique()) - 1)\n",
    "            s = temp.append(1-d)\n",
    "    ar = np.array(temp)\n",
    "    shape = (len(dframe_train), len(dframe_test))\n",
    "    ar_reshape = ar.reshape(shape,)\n",
    "    df_sim = pd.DataFrame(data = ar_reshape)\n",
    "    return(df_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average function :\n",
    "\n",
    "* Combines similarities for different attribute types using general approach. delta_k here is the delta values of attributes for which jaccardian similarity is applied. \n",
    "* Loop iterates through all (except jaccardian) the data frames containing similarity values for which average similarity is to be calculated.\n",
    "* The same is done with jaccardian, but if the two vectors are 0 (delta value for that element in df is 0) then it is not included in the average, delta_count makes sure of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_sim(df_list,delta_k):\n",
    "    #print(num_test_rows)\n",
    "    #print(delta_k)\n",
    "    average = []\n",
    "    length = num_rows\n",
    "    for i in range(0,num_test_rows):\n",
    "        for j in range(0,length):\n",
    "#             if(i > 286):\n",
    "#                 print(i,j,end=' ',flush=True)\n",
    "            avg = 0\n",
    "            delta_count = 0\n",
    "            for k in range(0,len(df_list)-1):\n",
    "                avg = avg + df_list[k][i][j]\n",
    "                delta_count = delta_count + 1\n",
    "            k = len(df_list)-1\n",
    "            if(k == len(df_list)-1):\n",
    "                avg = avg + df_list[k][i][j] * delta_k[i][j]\n",
    "                if(delta_k[i][j] == 1):\n",
    "                    delta_count = delta_count + 1\n",
    "            avg = avg / (delta_count + 2)\n",
    "            average.append(avg)\n",
    "    \n",
    "    ar = np.array(average)\n",
    "    shape = (length, num_test_rows)\n",
    "    ar_reshape = ar.reshape(shape,)\n",
    "    df_avg = pd.DataFrame(data = ar_reshape)\n",
    "    return(df_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Replacing the values that are other than 0 in capital_gain and capital_loss with 1.\n",
    "* This will help us to apply jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_positive_with_one(dframes,df):\n",
    "    for i in range(0,len(dframes)):\n",
    "        for j in range(0,len(df[dframes[i]])):\n",
    "            if(df[dframes[i]][j] > 0):\n",
    "                df[dframes[i]][j] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_diagnol(df_avg):\n",
    "    for i in range(0,num_rows):\n",
    "        df_avg[i][i] = 0\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manhattan(a,b):\n",
    "    return abs(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarities(df_train,df_test):\n",
    "    df_train_euc = df_train[[\"age\",\"fnlwgt\",\"hour_per_week\"]]\n",
    "    df_test_euc = df_test[[\"age\",\"fnlwgt\",\"hour_per_week\"]]\n",
    "    con_similarity=[]                                     #similarity for continuous variables age,fnlwgt\n",
    "    for i in df_train_euc.values:\n",
    "        for j in df_test_euc.values:\n",
    "            con_similarity.append(1/np.exp(mdist(i,j,df_train_euc)))   # Once we have the distance d, we can calculate the similarity by 1/e^d\n",
    "    ar = np.array(con_similarity)                         # This is the final array which consists of 270,400 values\n",
    "    shape = (num_rows,num_test_rows)\n",
    "    ar_new = ar.reshape(shape,)                           # Arranging the 1d array to 520 x 520 matrix\n",
    "    ar_new.shape\n",
    "    global df_mdist\n",
    "    df_mdist = pd.DataFrame(data=ar_new)                  # Loading these values into a dataframe\n",
    "    replace_dframes = [\"capital_loss\",\"capital_gain\"]\n",
    "    df_train = replace_positive_with_one(replace_dframes,df_train)\n",
    "    df_test = replace_positive_with_one(replace_dframes,df_test)\n",
    "    df_jac_sim,df_delta_k = jaccardian_sim(replace_dframes,df_train,df_test)\n",
    "    #print(df_jac_sim)\n",
    "    df_edu_sim = ord_sim(df_train[\"education_cat\"],df_test[\"education_cat\"])\n",
    "    df_occu_sim = nom_sim(df_train[\"occupation\"],df_test[\"occupation\"])\n",
    "    df_race_sim = nom_sim(df_train[\"race\"],df_test[\"race\"])\n",
    "    df_country_sim = nom_sim(df_train[\"native_country\"],df_test[\"native_country\"])\n",
    "    df_marital_sim = nom_sim(df_train[\"marital_status\"],df_test[\"marital_status\"])\n",
    "    df_gender_sim = nom_sim(df_train[\"gender\"],df_test[\"gender\"])\n",
    "    df_relationship_sim = nom_sim(df_train[\"relationship\"],df_test[\"relationship\"])\n",
    "    #print(df_mdist[288][56])\n",
    "    df_list = [df_mdist,df_edu_sim,df_occu_sim,df_race_sim,df_country_sim,df_marital_sim,df_gender_sim,df_relationship_sim,df_jac_sim]\n",
    "    df_avg = average_sim(df_list,df_delta_k)\n",
    "    #df_avg = replace_diagnol(df_avg)\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mdist[287][510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(training_file,test_file):\n",
    "    global df_train\n",
    "    df_train = pd.read_csv(training_file) #Reading the income_tr csv file into a dataframe df\n",
    "    global df_test\n",
    "    df_test = pd.read_csv(test_file)\n",
    "    df_test.drop(\"ID\",axis = 1, inplace=True)\n",
    "    df_train.drop(\"ID\",axis = 1, inplace=True)\n",
    "#     df_combined = pd.concat([df_train,df_test],axis=0)\n",
    "#     df_train = df_combined\n",
    "#     df_test = df_combined\n",
    "#     print(df_test)\n",
    "    global num_rows\n",
    "    num_rows = len(df_train.index)\n",
    "    df_train = replace_missing(df_train)\n",
    "    df_train = normalize(df_train)\n",
    "    #df_avg = get_similarities(df_train)\n",
    "    global num_test_rows\n",
    "    num_test_rows = len(df_test.index)\n",
    "    \n",
    "    #df.describe() \n",
    "    df_test = replace_missing(df_test)\n",
    "    df_test = normalize(df_test)\n",
    "    df_avg = get_similarities(df_train,df_test)\n",
    "    k = int(input('Enter the parameter k: '))\n",
    "# #df_prox = k_similar(df_avg,k)\n",
    "# df_temp = pd.DataFrame({'Transaction ID':list(range(1,num_rows+1))})\n",
    "# df_prox = pd.concat([df_temp,df_prox],axis=1)\n",
    "# df_prox.to_csv(\"output.csv\")\n",
    "# df_prox.head()\n",
    "    return df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the parameter k: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.680103</td>\n",
       "      <td>0.181994</td>\n",
       "      <td>0.230095</td>\n",
       "      <td>0.469878</td>\n",
       "      <td>0.477114</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.466891</td>\n",
       "      <td>0.666551</td>\n",
       "      <td>0.678740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617209</td>\n",
       "      <td>0.678855</td>\n",
       "      <td>0.435722</td>\n",
       "      <td>0.681552</td>\n",
       "      <td>0.687591</td>\n",
       "      <td>0.376740</td>\n",
       "      <td>0.247149</td>\n",
       "      <td>0.363203</td>\n",
       "      <td>0.584471</td>\n",
       "      <td>0.573273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.680103</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.185137</td>\n",
       "      <td>0.222348</td>\n",
       "      <td>0.358932</td>\n",
       "      <td>0.568529</td>\n",
       "      <td>0.371564</td>\n",
       "      <td>0.472083</td>\n",
       "      <td>0.654554</td>\n",
       "      <td>0.669381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706403</td>\n",
       "      <td>0.770113</td>\n",
       "      <td>0.526478</td>\n",
       "      <td>0.673234</td>\n",
       "      <td>0.787033</td>\n",
       "      <td>0.375919</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>0.360503</td>\n",
       "      <td>0.669238</td>\n",
       "      <td>0.465889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.181994</td>\n",
       "      <td>0.185137</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.323211</td>\n",
       "      <td>0.363746</td>\n",
       "      <td>0.279426</td>\n",
       "      <td>0.380344</td>\n",
       "      <td>0.382321</td>\n",
       "      <td>0.255132</td>\n",
       "      <td>0.167435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154401</td>\n",
       "      <td>0.166782</td>\n",
       "      <td>0.229130</td>\n",
       "      <td>0.270221</td>\n",
       "      <td>0.192586</td>\n",
       "      <td>0.387146</td>\n",
       "      <td>0.248480</td>\n",
       "      <td>0.255735</td>\n",
       "      <td>0.132170</td>\n",
       "      <td>0.276265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.230095</td>\n",
       "      <td>0.222348</td>\n",
       "      <td>0.323211</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.513733</td>\n",
       "      <td>0.422992</td>\n",
       "      <td>0.220470</td>\n",
       "      <td>0.313567</td>\n",
       "      <td>0.219185</td>\n",
       "      <td>0.212541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201448</td>\n",
       "      <td>0.229474</td>\n",
       "      <td>0.181356</td>\n",
       "      <td>0.228459</td>\n",
       "      <td>0.227133</td>\n",
       "      <td>0.217508</td>\n",
       "      <td>0.395870</td>\n",
       "      <td>0.315623</td>\n",
       "      <td>0.227342</td>\n",
       "      <td>0.325768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.469878</td>\n",
       "      <td>0.358932</td>\n",
       "      <td>0.363746</td>\n",
       "      <td>0.513733</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.571826</td>\n",
       "      <td>0.454642</td>\n",
       "      <td>0.452012</td>\n",
       "      <td>0.370750</td>\n",
       "      <td>0.355613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329291</td>\n",
       "      <td>0.355052</td>\n",
       "      <td>0.358092</td>\n",
       "      <td>0.355784</td>\n",
       "      <td>0.364598</td>\n",
       "      <td>0.368313</td>\n",
       "      <td>0.344586</td>\n",
       "      <td>0.338463</td>\n",
       "      <td>0.312744</td>\n",
       "      <td>0.565309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.800000  0.680103  0.181994  0.230095  0.469878  0.477114  0.468200   \n",
       "1  0.680103  0.800000  0.185137  0.222348  0.358932  0.568529  0.371564   \n",
       "2  0.181994  0.185137  0.800000  0.323211  0.363746  0.279426  0.380344   \n",
       "3  0.230095  0.222348  0.323211  0.800000  0.513733  0.422992  0.220470   \n",
       "4  0.469878  0.358932  0.363746  0.513733  0.800000  0.571826  0.454642   \n",
       "\n",
       "        7         8         9      ...          510       511       512  \\\n",
       "0  0.466891  0.666551  0.678740    ...     0.617209  0.678855  0.435722   \n",
       "1  0.472083  0.654554  0.669381    ...     0.706403  0.770113  0.526478   \n",
       "2  0.382321  0.255132  0.167435    ...     0.154401  0.166782  0.229130   \n",
       "3  0.313567  0.219185  0.212541    ...     0.201448  0.229474  0.181356   \n",
       "4  0.452012  0.370750  0.355613    ...     0.329291  0.355052  0.358092   \n",
       "\n",
       "        513       514       515       516       517       518       519  \n",
       "0  0.681552  0.687591  0.376740  0.247149  0.363203  0.584471  0.573273  \n",
       "1  0.673234  0.787033  0.375919  0.238176  0.360503  0.669238  0.465889  \n",
       "2  0.270221  0.192586  0.387146  0.248480  0.255735  0.132170  0.276265  \n",
       "3  0.228459  0.227133  0.217508  0.395870  0.315623  0.227342  0.325768  \n",
       "4  0.355784  0.364598  0.368313  0.344586  0.338463  0.312744  0.565309  \n",
       "\n",
       "[5 rows x 520 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file = \"income_tr.csv\"\n",
    "test_file = \"income_tr.csv\"\n",
    "global df_avg\n",
    "df_avg = main(training_file,test_file)\n",
    "df_avg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Diagonal Elements:\n",
    "Replace all the diagonal elements in the final similarity data frames with zero as we do not want to consider an objects similarity with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter \"k\" :\n",
    "* Get k similar objects for each object in the dataset. Get k largest elements and their indices using DataFrame.nlargest and DataFrame.nlargest.index function in each column (Similarity matrix is symmetric). Each column represents one object/record. Use these k values and indices to populate the final output dataframe.\n",
    "* After executing this step, it will ask you to enter the value for k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_similar(df_avg,k):\n",
    "    df_k_similar = []\n",
    "    temp_indices = []\n",
    "    temp_values = []\n",
    "    \n",
    "    for i in range(0,num_rows):\n",
    "        temp_indices = df_avg.nlargest(k,i).index\n",
    "        temp_indices = [x+1 for x in temp_indices]\n",
    "        temp_values = df_avg.nlargest(k,i)[i]\n",
    "        t = 1       \n",
    "        #df_prox = DataFrame({'Transaction ID':i+1})\n",
    "        for j in temp_values:\n",
    "            df_temp1 = pd.DataFrame({t:[temp_indices[t-1]]})\n",
    "            df_temp2 = pd.DataFrame({'%d-prox'%t:[j]})     \n",
    "            if(t == 1 ):\n",
    "                df_temp3 = pd.concat([df_temp1,df_temp2],axis=1)\n",
    "            else:\n",
    "                df_temp3 = pd.concat([df_temp3,df_temp1,df_temp2],axis=1)\n",
    "            t = t + 1\n",
    "        if(i == 0):\n",
    "            df_prox = df_temp3.copy()\n",
    "            #print(df_prox)\n",
    "        else:\n",
    "            df_prox = pd.concat([df_prox,df_temp3],axis=0,ignore_index=True)\n",
    "    return df_prox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section - C:\n",
    "\n",
    "### A.\n",
    "* To check the distribution of each proximation, we will plot histograms. This will help us to get an idea how the data are distributed. We plotted histograms all the way upto k= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_prox[\"1-prox\"].hist(bins = 20)\n",
    "# plt.title('k = 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_prox[\"2-prox\"].hist(bins = 20)\n",
    "# plt.title('k = 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_prox[\"3-prox\"].hist(bins = 20)\n",
    "# plt.title('k = 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_prox[\"4-prox\"].hist(bins = 20)\n",
    "# plt.title('k = 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_prox[\"5-prox\"].hist(bins = 20)\n",
    "# plt.title('k = 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_prox[\"10-prox\"].hist(bins = 20)\n",
    "# plt.title('k = 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. \n",
    "* We will concatenate the class attribute with df_prox dataframe to look for any differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.concat((df_prox, df[\"class\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once they are concatenated, we will see the distributions of similarities over the 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K = 2\n",
    "# df_test[\"1-prox\"].hist(by = df[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K=2\n",
    "# df_test[\"2-prox\"].hist(by = df[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test[\"3-prox\"].hist(by = df[\"class\"])\n",
    "# K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test[\"4-prox\"].hist(by = df[\"class\"])\n",
    "# K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test[\"5-prox\"].hist(by = df[\"class\"])\n",
    "# K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_test[\"10-prox\"].hist(by = df[\"class\"])\n",
    "# K =10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see in the graphs above, as we increased the value of k, class attribute with value <=50 tends to follow normal distribution.\n",
    "* Where as the class attribute value > 50 is still very much away from normality. We can check the significance of the distribution using statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Closest to the largest number:\n",
    "* This can be checked using value_counts() on k = 1, 3, 5 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K = 1\n",
    "# df_test[1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 15 is closest to few other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K = 3\n",
    "# df_test[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K = 5\n",
    "# df_test[5].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K = 10\n",
    "# df_test[10].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initially for k = 1, there are not many examples that are close to one particular example. But when we increased the k the number of examples that are closer is increasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN :\n",
    "* Cross validation with 8 folds will be used to test the data.\n",
    "* Records are shuffled randomly.\n",
    "* sub_indices are the records chosen for testing in one particular fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_k_nearest(df_class,df_avg,train_indices,test_indices,k):\n",
    "    #print(test_indices)\n",
    "    #print(train_indices)\n",
    "    df_train_avg = df_avg.iloc[train_indices]\n",
    "    #print(df_train_avg)\n",
    "    df_predicted_class = pd.DataFrame({\"Predicted Class\":[],\"Posterior\":[]})\n",
    "    for i in test_indices:\n",
    "        nearest_indices = df_train_avg.nlargest(k,i).index\n",
    "        #print(nearest_indices)\n",
    "        (predicted_class,posterior) = get_posterior_and_class(df_class,nearest_indices)\n",
    "        #print(predicted_class,posterior)\n",
    "        df_temp = pd.DataFrame({\"Predicted Class\":[predicted_class],\"Posterior\":[posterior]})\n",
    "        df_predicted_class = pd.concat([df_predicted_class,df_temp],axis = 0)\n",
    "    df_predicted_class = df_predicted_class.set_index(test_indices)\n",
    "    #print(df_predicted_class)\n",
    "    return df_predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior_and_class(df_class,nearest_indices):\n",
    "    c1_count = 0 # <=50K\n",
    "    c2_count = 0 # >50K\n",
    "    for j in nearest_indices:\n",
    "        if(df_class[j] == \" <=50K\"):\n",
    "            #print(\"Found class: \" + df_class[j])\n",
    "            c1_count = c1_count + 1\n",
    "            #print(c1_count)\n",
    "        else:\n",
    "            #print(\"Found class:\" + df_class[j])\n",
    "            c2_count = c2_count + 1\n",
    "            #print(c2_count)\n",
    "    #print(c1_count,c2_count,len(nearest_indices))\n",
    "    posterior_c1 = c1_count/len(nearest_indices)\n",
    "    posterior_c2 = c2_count/len(nearest_indices)\n",
    "    if(posterior_c1 > posterior_c2):\n",
    "        return (\" <=50K\",posterior_c1)\n",
    "    elif(posterior_c1 < posterior_c2):\n",
    "        return (\" >50K\",posterior_c2)\n",
    "    else:\n",
    "        return (\" <=50K\",posterior_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_confusion_matrix(df_output):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    for i in df_output.index:\n",
    "        if(df_output[\"Actual Class\"][i] == \" <=50K\" and df_output[\"Predicted Class\"][i] == \" <=50K\"):\n",
    "            true_positive = true_positive + 1\n",
    "        elif(df_output[\"Actual Class\"][i] == \" <=50K\" and df_output[\"Predicted Class\"][i] == \" >50K\"):\n",
    "            false_negative = false_negative + 1\n",
    "        elif(df_output[\"Actual Class\"][i] == \" >50K\" and df_output[\"Predicted Class\"][i] == \" <=50K\"):\n",
    "            false_positive = false_positive + 1\n",
    "        else:\n",
    "            true_negative = true_negative + 1\n",
    "    confusion_matrix = [[true_positive,false_negative],[false_positive,true_negative]]\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classification_accuracy(df_output,num_rows):\n",
    "    accuracy = 0\n",
    "    mse = 0\n",
    "    for i in df_output.index:\n",
    "        if(df_output[\"Actual Class\"][i] == df_output[\"Predicted Class\"][i]):\n",
    "            accuracy = accuracy + 1\n",
    "        else:\n",
    "            mse = mse + 1\n",
    "    accuracy = accuracy * 100/num_rows\n",
    "    mse = mse * 100/num_rows\n",
    "    return (accuracy,mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_main(cv,k,df_avg):\n",
    "    df_avg = df_avg.sample(frac=1)\n",
    "    indices = df_avg.index\n",
    "    #pint(indices)\n",
    "    i = 0;\n",
    "    df_temp = pd.DataFrame({\"Predicted Class\":[],\"Posterior\":[]})\n",
    "    while i != num_rows:\n",
    "        test_indices = indices[i:i+cv]\n",
    "        train_indices = list(range(0,i)) + list(range(i+cv,num_rows))\n",
    "        i = i + cv\n",
    "        df_predicted_class = get_k_nearest(df_train[\"class\"],df_avg,train_indices,test_indices,k)\n",
    "        df_temp = pd.concat([df_temp,df_predicted_class])\n",
    "    df_temp = df_temp.sort_index(axis=0)\n",
    "    df_class = df_train[\"class\"]\n",
    "    df_actual_class = pd.DataFrame({\"Actual Class\":df_class})\n",
    "    df_temp = pd.concat([df_actual_class,df_temp],axis=1)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_k_nearest_2(df_class,df_avg,k):\n",
    "    df_predicted_class = pd.DataFrame({\"Predicted Class\":[],\"Posterior\":[]})\n",
    "    for i in df_avg.columns:\n",
    "        nearest_indices = df_avg.nlargest(k,i).index\n",
    "        #print(nearest_indices)\n",
    "        (predicted_class,posterior) = get_posterior_and_class(df_class,nearest_indices)\n",
    "        #print(predicted_class,posterior)\n",
    "        df_temp = pd.DataFrame({\"Predicted Class\":[predicted_class],\"Posterior\":[posterior]})\n",
    "        df_predicted_class = pd.concat([df_predicted_class,df_temp],axis = 0) \n",
    "    test_indices = list(range(0,num_test_rows))\n",
    "    df_predicted_class = df_predicted_class.set_index([test_indices])\n",
    "    return df_predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_main2(k,df_avg):\n",
    "    df_temp = pd.DataFrame({\"Predicted Class\":[],\"Posterior\":[]})\n",
    "    #df_temp = pd.DataFrame({\"Predicted Class\":[],\"Posterior\":[]})\n",
    "    df_temp = get_k_nearest_2(df_train[\"class\"],df_avg,k)\n",
    "    #print(df_temp)\n",
    "    df_class = df_test[\"class\"]\n",
    "    #print(df_class)\n",
    "    df_actual_class = pd.DataFrame({\"Actual Class\":df_class})\n",
    "    df_temp = pd.concat([df_actual_class,df_temp],axis=1)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49]\n",
      "[[373, 47], [49, 51]]\n",
      "81.53846153846153 18.46153846153846\n",
      "[[385, 35], [57, 43]]\n",
      "82.3076923076923 17.692307692307693\n",
      "[[386, 34], [58, 42]]\n",
      "82.3076923076923 17.692307692307693\n",
      "[[396, 24], [54, 46]]\n",
      "85.0 15.0\n",
      "[[392, 28], [56, 44]]\n",
      "83.84615384615384 16.153846153846153\n",
      "[[397, 23], [59, 41]]\n",
      "84.23076923076923 15.76923076923077\n",
      "[[395, 25], [61, 39]]\n",
      "83.46153846153847 16.53846153846154\n",
      "[[396, 24], [60, 40]]\n",
      "83.84615384615384 16.153846153846153\n",
      "[[395, 25], [62, 38]]\n",
      "83.26923076923077 16.73076923076923\n",
      "[[396, 24], [61, 39]]\n",
      "83.65384615384616 16.346153846153847\n",
      "[[397, 23], [58, 42]]\n",
      "84.42307692307692 15.576923076923077\n",
      "[[398, 22], [62, 38]]\n",
      "83.84615384615384 16.153846153846153\n",
      "[[399, 21], [63, 37]]\n",
      "83.84615384615384 16.153846153846153\n",
      "[[401, 19], [64, 36]]\n",
      "84.03846153846153 15.961538461538462\n",
      "[[403, 17], [65, 35]]\n",
      "84.23076923076923 15.76923076923077\n",
      "[[403, 17], [66, 34]]\n",
      "84.03846153846153 15.961538461538462\n",
      "[[403, 17], [70, 30]]\n",
      "83.26923076923077 16.73076923076923\n",
      "[[404, 16], [70, 30]]\n",
      "83.46153846153847 16.53846153846154\n",
      "[[405, 15], [71, 29]]\n",
      "83.46153846153847 16.53846153846154\n",
      "[[402, 18], [70, 30]]\n",
      "83.07692307692308 16.923076923076923\n",
      "[[405, 15], [70, 30]]\n",
      "83.65384615384616 16.346153846153847\n",
      "[[405, 15], [69, 31]]\n",
      "83.84615384615384 16.153846153846153\n",
      "[[403, 17], [70, 30]]\n",
      "83.26923076923077 16.73076923076923\n",
      "[[403, 17], [70, 30]]\n",
      "83.26923076923077 16.73076923076923\n",
      "[[404, 16], [73, 27]]\n",
      "82.88461538461539 17.115384615384617\n"
     ]
    }
   ],
   "source": [
    "cv = 8\n",
    "k = list(range(1,30,2))\n",
    "print(k)\n",
    "for i in k:\n",
    "    df_temp = knn_main(cv,i,df_avg)\n",
    "    #df_temp = knn_main2(i,df_avg)\n",
    "    #print(df_temp)\n",
    "    confusion_matrix = get_confusion_matrix(df_temp)\n",
    "    (accuracy,mse) = get_classification_accuracy(df_temp,num_test_rows)\n",
    "    print(confusion_matrix)\n",
    "    print(i,accuracy,mse)\n",
    "#df_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
